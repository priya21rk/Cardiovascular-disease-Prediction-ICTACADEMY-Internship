# -*- coding: utf-8 -*-
"""ICT_CVD_(PROJECT_ML/AI).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P2CkT8gQLYBnRY3pPSUDMXDm2uXQfkDK

# **PREDICTION OF CARDIOVASCULAR DISEASES USING MACHINE LEARNING**

To predict the presence / absence of cardiovascular disease in a person if the features
are given

# **LOADING THE DATA**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import csv

data = pd.read_csv('cardio_train.csv')

"""# **EXPLORATORY DATA ANALYSIS**"""

#first few rows of the dataset
data.head()

#information regarding the data
data.info()

#Check for missing values in each column
data.isna().sum()

#dimension of the data
data.shape

#data types of each column
data.dtypes

#count the number of unique values in each column
data.nunique()

#summary statistics for numerical columns
data.describe()

"""# **ENCODING**

# **LABEL ENCODING**

converting categorical variables into numerical values
"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

data['gender'] = label_encoder.fit_transform(data['gender'])

data['active'] = label_encoder.fit_transform(data['active'])

data['active'] = label_encoder.fit_transform(data['active'])

data['cholesterol'] = label_encoder.fit_transform(data['cholesterol'])

data['gluc'] = label_encoder.fit_transform(data['gluc'])

data['active'] = label_encoder.fit_transform(data['active'])

data['alco'] = label_encoder.fit_transform(data['alco'])

data['smoke'] = label_encoder.fit_transform(data['smoke'])

"""Converting age in days to age in years."""

data.age = (data.age / 365.25)

data.age = data.age.astype(int)

data.head()

data.tail()

"""# **PATIENTS WITH AND WITHOUT CVD**"""

data.cardio.value_counts()

sns.countplot(x='cardio', data=data)
plt.xlabel('CVD Status')
plt.ylabel('Count')
plt.title('Count of Patients with and without CVD')
plt.show()

"""# **HISTOGRAM BASED ON FEATURES GIVEN**"""

freqgraph = data.select_dtypes(include=['float', 'int'])
freqgraph.hist(figsize=(20,15))
plt.show()

"""# **CVD FREQUENCY BASED ON GENDER**

male - 1


female - 0
"""

data.gender.value_counts()

pd.crosstab(data.cardio, data.gender)

pd.crosstab(data.cardio, data.gender).plot(kind="bar", figsize=(10,6), color=["pink", "lightblue"]);
plt.title("CVD Frequency based on gender")
plt.xlabel("0 = NO CVD, 1 = CVD")
plt.ylabel("Count")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);

"""# **CVD FREQUENCY BASED ON CHOLESTEROL LEVEL**

normal - 0


above normal - 1


well above normal - 2
"""

pd.crosstab(data.cholesterol, data.cardio)

pd.crosstab(data.cholesterol, data.cardio).plot(kind="bar", figsize=(10,6), color=["pink", "lightblue"])
plt.title("CVD frequency per cholesterol level")
plt.xlabel("Cholesterol level")
plt.ylabel("Frequency")
plt.legend(["No CVD", "CVD"])
plt.xticks(rotation = 0);

"""# **AGE AND SYSTOLIC BLOOD PRESSURE FOR CVD**"""

plt.figure(figsize=(10,6))
plt.scatter(data.age[data.cardio==1], data.ap_hi[data.cardio==1], c="pink") 
plt.scatter(data.age[data.cardio==0], data.ap_hi[data.cardio==0], c="lightblue") 
plt.title("CVD as a function of age and systolic blood pressure")
plt.xlabel("Age")
plt.legend(["No CVD", "CVD"])
plt.ylabel("systolic blood pressure");

"""# **CORRELATION MATRIX**"""

data = pd.DataFrame(data)
data.corr()

"""# **HEATMAP**"""

plt.figure(figsize=(20,12))
sns.set_context('notebook',font_scale = 1.3)
sns.heatmap(data.corr(),annot=True,linewidth =2)
plt.tight_layout()

"""# **MISSING VALUES HANDLING**"""

data.isna().sum()
#no missing values found

"""# **OUTLIERS DETECTION**

Outliers are data points that deviate significantly from the majority of the data.

Outliers can occur due to various reasons such as measurement errors, data entry errors, or genuine extreme values.
"""

#displays the columns of the dataset
data.columns

"""# **BOXPLOT**"""

for i in ['ap_hi', 'ap_lo', 'age', 'height', 'weight']:
    plt.figure()
    plt.boxplot(data[i])
    plt.title(i)

"""# AP_HI(SYSTOLIC BLOOD PRESSURE)"""

Q1 = np.percentile(data['ap_hi'], 25, interpolation = 'midpoint')
Q2 = np.percentile(data['ap_hi'], 50, interpolation = 'midpoint')
Q3 = np.percentile(data['ap_hi'], 75, interpolation = 'midpoint')

print(Q1)
print(Q2)
print(Q3)

data['ap_hi'].median()

IQR=Q3-Q1

low_lim = Q1 -1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier =[]
for x in data['ap_hi']:
  if((x>up_lim) or (x<low_lim)):
    outlier.append(x)

outlier

ind1 = data['ap_hi']<low_lim
data.loc[ind1].index

ind1 = data['ap_hi'] < low_lim
values = data.loc[ind1].index.tolist()
values

data.drop([209,
 383,
 567,
 636,
 927,
 979,
 1600,
 1627,
 1772,
 2167,
 2203,
 2431,
 2528,
 2612,
 2853,
 2990,
 3447,
 3449,
 3623,
 3683,
 3834,
 3846,
 3858,
 3905,
 3949,
 4280,
 4347,
 4382,
 4465,
 4582,
 4595,
 4607,
 4616,
 4685,
 4830,
 4941,
 4983,
 5225,
 5382,
 5712,
 5760,
 6569,
 7076,
 7374,
 7506,
 7710,
 7949,
 8141,
 8280,
 8349,
 8422,
 8757,
 8947,
 9285,
 9298,
 9452,
 9735,
 9897,
 10232,
 10723,
 11050,
 11060,
 11102,
 11146,
 11454,
 11577,
 11793,
 11951,
 13172,
 13675,
 13709,
 13755,
 13883,
 14127,
 14269,
 14453,
 14500,
 14755,
 15704,
 16021,
 16340,
 16479,
 16629,
 17248,
 17277,
 17328,
 17529,
 17872,
 17929,
 17939,
 18096,
 18514,
 18877,
 19044,
 19363,
 19457,
 20192,
 20536,
 20694,
 20697,
 20777,
 20886,
 20917,
 20954,
 21425,
 22075,
 22510,
 22670,
 22772,
 22826,
 23703,
 23988,
 24431,
 24537,
 24548,
 24557,
 24672,
 24707,
 24825,
 24850,
 25117,
 25240,
 25454,
 25785,
 25812,
 26491,
 26513,
 26630,
 26991,
 27378,
 27502,
 28024,
 28026,
 28089,
 28113,
 28184,
 28392,
 28466,
 28629,
 29170,
 29444,
 29510,
 29985,
 30260,
 30382,
 30766,
 30795,
 30930,
 31212,
 31315,
 32733,
 32797,
 33103,
 33493,
 33749,
 33915,
 34395,
 34427,
 34516,
 34528,
 34655,
 34743,
 35040,
 35308,
 35410,
 35566,
 36015,
 36305,
 36687,
 37329,
 37392,
 37455,
 37564,
 38250,
 38271,
 38627,
 38730,
 39240,
 39355,
 39409,
 39730,
 40454,
 40525,
 40634,
 40899,
 41503,
 41727,
 41933,
 42009,
 42138,
 42274,
 42334,
 42461,
 42656,
 42991,
 43128,
 43233,
 43484,
 43697,
 44051,
 44502,
 44844,
 44870,
 45019,
 45232,
 45446,
 45551,
 45951,
 46031,
 46155,
 46160,
 46608,
 46627,
 46685,
 46820,
 47166,
 47308,
 47407,
 47485,
 47696,
 47769,
 48290,
 48385,
 48839,
 48851,
 48881,
 49201,
 49736,
 49753,
 49886,
 50037,
 50888,
 51073,
 51650,
 52067,
 52580,
 52690,
 52851,
 53597,
 54045,
 54223,
 54447,
 54470,
 54517,
 54945,
 54957,
 55007,
 55043,
 55047,
 55126,
 55255,
 55256,
 55393,
 55412,
 55911,
 56108,
 56437,
 56635,
 56777,
 56825,
 56927,
 57236,
 57331,
 57364,
 57482,
 57786,
 57909,
 57926,
 57935,
 58013,
 58188,
 59044,
 59283,
 59958,
 59973,
 60106,
 60275,
 60578,
 60727,
 60922,
 61380,
 62176,
 62188,
 62261,
 62458,
 62719,
 62817,
 63077,
 63268,
 63698,
 63715,
 63913,
 64352,
 64409,
 64444,
 64454,
 64556,
 64626,
 65344,
 65685,
 65758,
 66123,
 66315,
 66657,
 66740,
 67137,
 67368,
 67421,
 67470,
 67658,
 67947,
 68021,
 68067,
 68455,
 68630,
 68665,
 68742,
 68998,
 69137,
 69265,
 69549],inplace=True)

outliers = data[(data['ap_hi'] < low_lim) | (data['ap_hi'] > up_lim)]

data = data[(data['ap_hi'] >= low_lim) & (data['ap_hi'] <= up_lim)]

for i in [ 'ap_hi']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""# AP_LO(DISTOLIC BLOOD PRESSURE)"""

Q1 = np.percentile(data['ap_lo'],25,interpolation ='midpoint')
Q2 = np.percentile(data['ap_lo'],50,interpolation ='midpoint')
Q3 = np.percentile(data['ap_lo'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)

IQR = Q3-Q1

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['ap_lo']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['ap_lo']<low_lim
data.loc[ind1].index

ind1 = data['ap_lo'] < low_lim
values = data.loc[ind1].index.tolist()
values

outliers = data[(data['ap_lo'] < low_lim) | (data['ap_lo'] > up_lim)]

data = data[(data['ap_lo'] >= low_lim) & (data['ap_lo'] <= up_lim)]

for i in [ 'ap_lo']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""# HEIGHT"""

Q1 = np.percentile(data['height'],25,interpolation ='midpoint')
Q2 = np.percentile(data['height'],50,interpolation ='midpoint')
Q3 = np.percentile(data['height'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)

IQR = Q3-Q1

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['height']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['height']<low_lim
data.loc[ind1].index

ind1 = data['height'] < low_lim
values = data.loc[ind1].index.tolist()
values

data.drop([224,
 249,
 781,
 1048,
 2412,
 2663,
 2944,
 3132,
 3208,
 3420,
 3735,
 3752,
 4212,
 5326,
 5774,
 6153,
 6303,
 6821,
 6997,
 7058,
 7116,
 7305,
 8171,
 8677,
 8760,
 9131,
 9284,
 9776,
 10095,
 10116,
 10567,
 11183,
 11230,
 12435,
 12770,
 12980,
 13029,
 13195,
 13265,
 13448,
 13870,
 13952,
 14073,
 14291,
 14323,
 14410,
 15021,
 15167,
 16677,
 16699,
 17074,
 17316,
 17988,
 18062,
 18092,
 19421,
 19672,
 20146,
 20225,
 22239,
 22253,
 22523,
 22723,
 22818,
 22838,
 23024,
 23777,
 23913,
 24006,
 24268,
 24979,
 25036,
 26009,
 26108,
 26931,
 27120,
 27384,
 27603,
 28099,
 28448,
 28513,
 28737,
 28817,
 28897,
 28975,
 29157,
 29567,
 29735,
 29740,
 29894,
 30279,
 30293,
 30327,
 30341,
 30907,
 31042,
 31142,
 31420,
 31657,
 31918,
 32098,
 32128,
 32589,
 32879,
 33534,
 33607,
 34169,
 34241,
 34276,
 34278,
 34288,
 34364,
 34860,
 35080,
 35433,
 35444,
 35774,
 35793,
 35799,
 36727,
 36753,
 37388,
 38330,
 38360,
 39079,
 39098,
 39543,
 39578,
 39962,
 40326,
 40732,
 40965,
 41268,
 41807,
 42592,
 42661,
 43759,
 44217,
 44388,
 44447,
 44490,
 44824,
 44999,
 45091,
 45398,
 46043,
 46173,
 46203,
 46233,
 46319,
 46374,
 46572,
 46750,
 47298,
 47299,
 47352,
 47925,
 49079,
 49320,
 49596,
 49693,
 49819,
 50136,
 50617,
 50789,
 51043,
 51093,
 51459,
 51837,
 51849,
 51909,
 51913,
 52742,
 52758,
 52962,
 53119,
 53344,
 54500,
 54688,
 54918,
 55387,
 56051,
 56123,
 56518,
 56669,
 56867,
 56925,
 56956,
 57081,
 57369,
 57965,
 58581,
 59437,
 59453,
 59620,
 59625,
 59738,
 59854,
 59972,
 59977,
 60691,
 60838,
 61453,
 61602,
 61753,
 63130,
 63739,
 63751,
 64115,
 64562,
 64787,
 65297,
 66023,
 66090,
 66479,
 66643,
 66965,
 67062,
 68325,
 69052,
 69124,
 69430,
 69784],inplace=True)

outliers = data[(data['height'] < low_lim) | (data['height'] > up_lim)]

data = data[(data['height'] >= low_lim) & (data['height'] <= up_lim)]

for i in [ 'height']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""# WEIGHT"""

Q1 = np.percentile(data['weight'],25,interpolation ='midpoint')
Q2 = np.percentile(data['weight'],50,interpolation ='midpoint')
Q3 = np.percentile(data['weight'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)

IQR = Q3-Q1

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['weight']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['weight']<low_lim
data.loc[ind1].index

data.drop([5794, 10447, 10627, 11876, 14722, 16322, 16906, 18559, 19582,
            22016, 25198, 26806, 29333, 29488, 33478, 33511, 33817, 33820,
            34282, 35314, 40612, 41353, 44138, 44622, 48613, 51411, 51544,
            54682, 55339, 55852, 58200, 60188, 60699, 65650],inplace=True)

outliers = data[(data['weight'] < low_lim) | (data['weight'] > up_lim)]

data = data[(data['weight'] >= low_lim) & (data['weight'] <= up_lim)]

for i in [ 'weight']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""# **AGE**"""

Q1 = np.percentile(data['age'],25,interpolation ='midpoint')
Q2 = np.percentile(data['age'],50,interpolation ='midpoint')
Q3 = np.percentile(data['age'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)

IQR = Q3-Q1

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['age']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['age'] < low_lim
values = data.loc[ind1].index.tolist()

data = data[(data['age'] >= low_lim) & (data['age'] <= up_lim)]

for i in [ 'age']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

for i in ['ap_hi', 'ap_lo', 'age', 'height', 'weight']:
    plt.figure()
    plt.boxplot(data[i])
    plt.title(i)

data.shape

"""## SCALING

the process of transforming and normalizing the features of a dataset to a consistent range

performed to ensure that all features have similar magnitudes and units, which can improve the performance of certain machine learning algorithms or models
"""

x=data.drop('cardio',axis=1)
y=data['cardio']

"""# **MIN MAX SCALING**


"""

x.describe()

x1=x.drop('id',axis=1)

x1.head()

from sklearn.preprocessing import MinMaxScaler
min_max_scaler=MinMaxScaler(feature_range=(0,1))
x1=min_max_scaler.fit_transform(x1)

type(x1)

x.columns

x1=pd.DataFrame(x1,columns=['age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo',
       'cholesterol', 'gluc', 'smoke', 'alco', 'active'])

x1.describe()

x1.shape

x1.head()

"""# **MODELLING**"""

X=data.drop('cardio',axis=1)
y=data['cardio']

X

y

"""# LINEAR REGRESSION"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.25)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
model = lr.fit(X_train, y_train)
prediction = model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

print('Mean squared error is:',mean_squared_error(y_test,prediction))
print('R squared value is:',r2_score(y_test,prediction))

"""# DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
dt_clf=DecisionTreeClassifier()
dt_clf.fit(X_train,y_train)
y_pred=dt_clf.predict(X_test)

report = classification_report(y_test, y_pred)
print(report)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(confusion_matrix(y_test, y_pred))

"""# LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(X_train,y_train)
y_pred=lr.predict(X_test)

report = classification_report(y_test, y_pred)
print(report)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(confusion_matrix(y_test, y_pred))

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(X_train,y_train)
y_pred=knn.predict(X_test)

report = classification_report(y_test, y_pred)
print(report)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(confusion_matrix(y_test, y_pred))

"""# RANDOM FOREST """

from sklearn.ensemble import RandomForestClassifier
rf_clf=RandomForestClassifier()
rf_clf.fit(X_train,y_train)

y_pred=rf_clf.predict(X_test)

report = classification_report(y_test, y_pred)
print(report)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(confusion_matrix(y_test,y_pred))

"""# NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB
naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

y_pred = naive_bayes.predict(X_test)

report = classification_report(y_test, y_pred)
print(report)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(confusion_matrix(y_test,y_pred))

from sklearn.model_selection import cross_val_score, KFold

# Define the models you want to evaluate
models = [
    ('Logistic Regression', lr),
    ('Decision Tree',dt_clf ),
    ('Random Forest', rf_clf),
    ('K-Nearest Neighbors',knn),
    ('Naive Bayes', naive_bayes)
]
model_names = []
mean_scores = []
std_scores = []

k = 10  # Number of folds
cv = KFold(n_splits=k, shuffle=True, random_state=42)  # Create KFold object
for model_name, model in models:
  
  
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

    model_names.append(model_name)
    mean_scores.append(scores.mean())
    std_scores.append(scores.std())

    print(f'{model_name}:')
    print('Accuracy:', scores)
    print('Mean Accuracy:', scores.mean())
    print('Standard Deviation:', scores.std())
    print()

"""# **HYPERPARAMETER TUNING**

# TUNING DECISION TREE WITH RANDOMISED SEARCH CV
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(5, 20),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5)
}

dt_classifier = DecisionTreeClassifier()

random_search = RandomizedSearchCV(estimator=dt_classifier, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy')

random_search.fit(X_train, y_train)

print("Best Hyperparameters:", random_search.best_params_)
print("Best Accuracy:", random_search.best_score_)

"""# TUNING DECISION TREE WITH GRID SEARCH CV"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

dt_classifier = DecisionTreeClassifier()

grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

"""# TUNING LOGISTIC REGRESSION WITH GRID SEARCH CV"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

classifier = LogisticRegression()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'logisticregression__C': [0.1, 1, 10],
    'logisticregression__penalty': ['l1', 'l2'],
    'logisticregression__solver': ['liblinear', 'saga']
}

# Create the pipeline with MinMaxScaler and the classifier
pipeline = Pipeline([
    ('minmaxscaler', MinMaxScaler()),
    ('logisticregression', classifier)
])

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Model:", grid_search.best_estimator_)

test_accuracy = best_model.score(X_test, y_test)
print("Test Accuracy:", test_accuracy)

"""# TUNING K-NEIGHBOR CLASSIFIER WITH RANDOMIZED SEARCH CV"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the parameter grid
param_grid = {
    'n_neighbors': randint(1, 20),       # Number of neighbors to consider
    'weights': ['uniform', 'distance'],  # Weight function used in prediction
    'p': [1, 2]                          # Power parameter for the Minkowski distance metric
}

knn_classifier = KNeighborsClassifier()

random_search = RandomizedSearchCV(
    estimator=knn_classifier,
    param_distributions=param_grid,
    n_iter=10,                      # Number of parameter settings that are sampled
    cv=5,                           # Number of cross-validation folds
    scoring='accuracy',             # Scoring metric for evaluation
    random_state=42
)

random_search.fit(X_train, y_train)

print("Best Hyperparameters:", random_search.best_params_)
print("Best Accuracy:", random_search.best_score_)

"""# TUNING NAIVE BAYES WITH GRID SEARCH CV"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV

param_grid = {
    'var_smoothing': [1e-9, 1e-8, 1e-7]  # Smoothing parameter for variance estimation
}

nb_classifier = GaussianNB()

grid_search = GridSearchCV(
    estimator=nb_classifier,
    param_grid=param_grid,
    cv=5,                # Number of cross-validation folds
    scoring='accuracy'   # Scoring metric for evaluation
)

grid_search.fit(X_train, y_train)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

import pickle

with open('classifier.pkl', 'wb') as file:
    pickle.dump(classifier, file)